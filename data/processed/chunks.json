[
  {
    "id": 0,
    "text": "ï»¿============================== TOPIC: Large Language Models ============================== Large Language Models (LLMs) are AI systems trained on large amounts of text data to generate human-like language. LLMs have several important limitations. One limitation is hallucination: LLMs may generate fluent answers that are factually incorrect or unsupported. Another limitation is outdated knowledge, since most LLMs are trained on static datasets. LLMs also lack true reasoning and understanding, as they rely on statistical patterns rather than real comprehension. Bias is another limitation, because training data can contain social or cultural biases. LLMs are also sensitive to prompt wording, meaning small prompt changes can lead to different outputs. ============================== TOPIC: Ret",
    "source": "rag_knowledge_base.txt"
  },
  {
    "id": 1,
    "text": "to prompt wording, meaning small prompt changes can lead to different outputs. ============================== TOPIC: Retrieval-Augmented Generation (RAG) ============================== Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. In a RAG system, the process works as follows: First, the user question is converted into an embedding. Second, relevant text chunks are retrieved from a document store using vector search methods such as FAISS. Third, the retrieved chunks are provided as context to a language model. Finally, the language model generates an answer grounded in the retrieved context. RAG helps reduce hallucinations and allows LLMs to answer questions using private or domain-specific documents. =========================",
    "source": "rag_knowledge_base.txt"
  },
  {
    "id": 2,
    "text": "hallucinations and allows LLMs to answer questions using private or domain-specific documents. ============================== TOPIC: Embeddings and Vector Search ============================== Embeddings are numerical vector representations of text that capture semantic meaning. Vector search compares embeddings to find text that is semantically similar to a query. FAISS is a library designed for efficient similarity search over large collections of embedding vectors. Vector search enables fast retrieval of relevant document chunks in RAG systems. ============================== TOPIC: Cross-Encoder Reranking ============================== A cross-encoder reranker improves retrieval quality by jointly encoding the query and each retrieved chunk. Unlike bi-encoders, cross-encoders consider d",
    "source": "rag_knowledge_base.txt"
  },
  {
    "id": 3,
    "text": "retrieval quality by jointly encoding the query and each retrieved chunk. Unlike bi-encoders, cross-encoders consider direct interactions between the query and the document text. Cross-encoder reranking is usually applied after initial vector search because it is more accurate but slower. ============================== TOPIC: Hallucination Prevention in RAG ============================== Hallucination prevention in RAG systems is achieved by grounding generation in retrieved documents. If relevant information is not found in the document store, the system should avoid generating an answer. Instead, a well-designed RAG system responds that the answer is not available in the provided documents. Similarity score thresholds are commonly used to decide whether retrieved content is reliable eno",
    "source": "rag_knowledge_base.txt"
  },
  {
    "id": 4,
    "text": "he provided documents. Similarity score thresholds are commonly used to decide whether retrieved content is reliable enough to answer a question.",
    "source": "rag_knowledge_base.txt"
  }
]